{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ca0a4",
   "metadata": {},
   "source": [
    "## 10-1 어텐션 매커니즘과 transformer  \n",
    "단순 순환 신경망의 단점을 극복하고자 LSTM과 GRU 등이 등장했지만 여전히 완벽한 해결책은 되지 못함!!  \n",
    "이러한 한계는 machine translation application에서도 두드러진다! 번역할 문장이 길어질수록 기존 RNN 기반  \n",
    "모델은 번역의 품질을 유지하기 어려워진다! 기계 번역에 사용되는 신경망 구조는 전형직으로 Seq2Seq 구조를 가진다!  \n",
    "시퀀스-투-시퀀스 작업은 일련의 텍스트를 입력받아 일련의 텍스트를 출력하는 작업이다. 대표적인 예로는 기계번역과 문서 요약이 있음ㅇㅇ  \n",
    "이런 작업을 수행하기 위해서 보통 Encoder-Decoder 구조가 사용된다.  \n",
    "Encoder와 Decoder는 Text를 한 토큰씩 처리한다. 입력할 때도 한 토큰씩 받고, 출력할 때도 한 token씩 생성해야 하므로 속도가 느리다!  \n",
    "예를 들어, Decoder는 I라는 토큰을 생성한 후, Encoder의 마지막 은닉 상태와 자신의 은닉 상태를 활용해 love를 만든다!  \n",
    "I love를 생성한 후 같은 방식으로 Encoder의 마지막 은닉 상태와 자신의 은닉 상태를 활용해 you를 출력한다.  \n",
    "이처럼 Decoder는 이전에 생성한 토큰을 참고하면서 다음 토큰을 생성하는데 이를 Auto-Regressive model이라고 한다!  \n",
    "즉, 자기회귀 모델이라는 뜻이다! 아무튼 이러다가 2014년 Attention mechanism이 등장했다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a65d71",
   "metadata": {},
   "source": [
    "#### Attention mechanism  \n",
    "순환 신경망 기반 Encoder-Decoder 모델의 성능을 크게 향상시킨 기술이다! 기존에는 Decoder가 Encoder의 마지막 은닉 상태만 참고하여  \n",
    "번역을 수행했지만, Attention mechanism을 사용하면 Encoder의 모든 time-step에서 계산된 은닉 상태를 활용할 수 있다!  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
